// Load environment variables from .env file
import "dotenv/config";

import { getAllLLMProviders } from "./src/llms";
import { cleanTmpDir, loadContextFile } from "./src/utils/file";
import {
  runAllTestsHumanEval,
  saveBenchmarkResults,
  loadTestDefinitions,
} from "./src/utils/test-manager";
import type { HumanEvalResult } from "./src/utils/humaneval";
import { ensureRequiredDirectories } from "./src/utils/ensure-dirs";
import path from "path";

/**
 * Parse command line arguments
 * @returns Parsed command line arguments
 */
function parseCommandLineArgs(): {
  contextFile?: string;
} {
  const args = process.argv.slice(2);
  let contextFile: string | undefined;

  // Parse arguments
  for (let i = 0; i < args.length; i++) {
    if (args[i] === "--context" && i + 1 < args.length) {
      contextFile = args[i + 1];
      i++; // Skip the next argument as it's the value for --context
    }
  }

  return {
    contextFile,
  };
}

/**
 * Main function to run the benchmark
 */
async function runBenchmark() {
  console.log("ğŸš€ Starting SvelteBench with HumanEval methodology...");

  try {
    // Parse command line arguments
    const { contextFile } = parseCommandLineArgs();

    // Load context file if specified
    let contextContent = "";
    if (contextFile) {
      try {
        // Resolve path relative to the current working directory
        const contextFilePath = path.resolve(process.cwd(), contextFile);
        contextContent = await loadContextFile(contextFilePath);
        console.log(`ğŸ‘‰ Using context file: ${contextFilePath}`);
      } catch (error) {
        console.error(`Error loading context file: ${error}`);
        process.exit(1);
      }
    }

    // Ensure required directories exist
    await ensureRequiredDirectories();

    // Clean base tmp directory
    await cleanTmpDir();

    // Check if we're in debug mode
    const isDebugMode = process.env.DEBUG_MODE === "true";

    // Get all available LLM providers and models
    console.log("ğŸ‘‰ Discovering available LLM providers and models...");
    const providerModels = await getAllLLMProviders();

    if (providerModels.length === 0) {
      throw new Error("No LLM provider/model combinations found");
    }

    console.log(
      `ğŸ‘‰ Found ${providerModels.length} provider/model combinations`
    );

    // Filter providers in debug mode
    let selectedProviderModels = providerModels;

    if (isDebugMode) {
      console.log("ğŸ› Running in DEBUG_MODE");

      // Get debug settings
      const debugProvider = process.env.DEBUG_PROVIDER;
      const debugModel = process.env.DEBUG_MODEL;

      // Filter by provider if specified
      if (debugProvider) {
        const matchingProviders = providerModels.filter(
          (pm) => pm.name.toLowerCase() === debugProvider.toLowerCase()
        );

        if (matchingProviders.length === 0) {
          console.error(
            `âŒ Provider "${debugProvider}" not found. Available providers: ${[...new Set(providerModels.map(pm => pm.name))].join(', ')}`
          );
          process.exit(1);
        } else {
          selectedProviderModels = matchingProviders;

          // Filter by model(s) if specified
          if (debugModel) {
            // Parse comma-separated list of models
            const requestedModels = debugModel
              .split(",")
              .map((m) => m.trim())
              .filter((m) => m.length > 0);

            if (requestedModels.length > 0) {
              const matchingModels = matchingProviders.filter((pm) =>
                requestedModels.some(
                  (requestedModel) =>
                    pm.modelId.toLowerCase() === requestedModel.toLowerCase()
                )
              );

              if (matchingModels.length > 0) {
                selectedProviderModels = matchingModels;
                console.log(
                  `ğŸ‘‰ Selected models: ${matchingModels
                    .map((m) => m.modelId)
                    .join(", ")}`
                );
              } else {
                console.warn(
                  `âš ï¸ None of the requested models "${debugModel}" found for provider "${debugProvider}"`
                );
                throw new Error(
                  `No matching models found for provider "${debugProvider}"`
                );
              }
            }
          } else {
            throw new Error(
              `No model specified for provider "${debugProvider}". Use DEBUG_MODEL to specify models.`
            );
          }
        }
      } else {
        // No provider specified, use first provider/model
        selectedProviderModels = [providerModels[0]];
      }

      console.log(
        `ğŸ‘‰ Selected provider: ${selectedProviderModels[0].name} (${
          selectedProviderModels.length === 1
            ? selectedProviderModels[0].modelId
            : `${selectedProviderModels.length} models`
        })`
      );
    }

    const debugTest = process.env.DEBUG_TEST;

    // Load test definitions based on debug mode
    let testDefinitions = undefined;
    if (isDebugMode) {
      const allTests = await loadTestDefinitions();

      if (allTests.length === 0) {
        throw new Error("No tests found");
      }

      if (debugTest) {
        const matchingTest = allTests.find((test) => test.name === debugTest);
        if (matchingTest) {
          testDefinitions = [matchingTest];
          console.log(`ğŸ‘‰ Selected test: ${matchingTest.name}`);
        } else {
          console.warn(`âš ï¸ Test "${debugTest}" not found, using all tests`);
          testDefinitions = undefined; // Use all tests
        }
      } else {
        // No test specified, use all tests
        testDefinitions = undefined;
        console.log(`ğŸ‘‰ Using all available tests`);
      }
    }

    // Set number of samples (use 10 samples by default unless a specific test was requested)
    const numSamples = debugTest ? 1 : 10;

    console.log(
      `ğŸ‘‰ Running with ${numSamples} samples per test (for pass@k metrics)`
    );

    const allResults: HumanEvalResult[] = [];

    // Group provider models by provider name
    const providerGroups: Record<string, typeof selectedProviderModels> = {};
    for (const providerModel of selectedProviderModels) {
      if (!providerGroups[providerModel.name]) {
        providerGroups[providerModel.name] = [];
      }
      providerGroups[providerModel.name].push(providerModel);
    }

    const providerNames = Object.keys(providerGroups);
    console.log(
      `\nğŸ‘‰ Running tests with ${providerNames.length} providers in parallel...`
    );

    // Create a promise for each PROVIDER (not model)
    const providerPromises = providerNames.map(async (providerName) => {
      const providerResults: HumanEvalResult[] = [];
      const providerModels = providerGroups[providerName];

      console.log(`\nğŸ‘‰ Starting tests with ${providerName}...`);

      // Run each MODEL sequentially within this provider
      for (const providerWithModel of providerModels) {
        try {
          console.log(`\nğŸ‘‰ Running model: ${providerWithModel.modelId}...`);

          // Ensure provider-specific tmp directory exists and is clean
          await cleanTmpDir(providerWithModel.name);

          // Determine number of samples for this model
          // Use only 1 sample for expensive o1-pro models
          const modelNumSamples = providerWithModel.modelId.startsWith("o1-pro") ? 1 : numSamples;
          
          if (modelNumSamples !== numSamples) {
            console.log(`  âš ï¸  Using ${modelNumSamples} sample${modelNumSamples > 1 ? 's' : ''} for expensive model`);
          }

          // Run tests with this provider model using HumanEval methodology
          const results = await runAllTestsHumanEval(
            providerWithModel.provider,
            modelNumSamples,
            testDefinitions, // Pass specific tests if in debug mode
            contextContent // Pass context content if available
          );

          // Add the results
          providerResults.push(...results);

          // Save individual model results immediately to prevent loss if later models fail
          if (results.length > 0) {
            try {
              const modelFilename = `${providerWithModel.name}-${providerWithModel.modelId}`;
              await saveBenchmarkResults(results, contextFile, contextContent, modelFilename);
              console.log(`ğŸ’¾ Saved individual results for ${providerWithModel.modelId}`);
            } catch (saveError) {
              console.error(`âš ï¸  Failed to save individual results for ${providerWithModel.modelId}:`, saveError);
              // Don't fail the entire run, just log and continue
            }
          }

          // Clean provider-specific tmp directory after tests
          await cleanTmpDir(providerWithModel.name);
        } catch (error) {
          console.error(
            `Error running tests with ${providerWithModel.name} (${providerWithModel.modelId}):`,
            error
          );
          // Continue with the next model rather than breaking the whole process
        }
      }

      return providerResults;
    });

    // Wait for all provider promises to complete
    const resultsArrays = await Promise.all(providerPromises);

    // Combine all results
    for (const results of resultsArrays) {
      allResults.push(...results);
    }

    // Save combined benchmark results with context information if available
    // This provides a single file with all results for easy comparison
    if (allResults.length > 0) {
      try {
        await saveBenchmarkResults(allResults, contextFile, contextContent);
        console.log(`ğŸ’¾ Saved combined results for all models`);
      } catch (saveError) {
        console.error(`âš ï¸  Failed to save combined results:`, saveError);
        // Don't fail the run, individual model results were already saved
      }
    }

    // Print summary
    console.log(`\nğŸ“Š ${isDebugMode ? "Debug" : "Benchmark"} Summary:`);
    console.log("===========================================");

    // Group results by test name
    const resultsByTest: Record<string, HumanEvalResult[]> = {};
    for (const result of allResults) {
      if (!resultsByTest[result.testName]) {
        resultsByTest[result.testName] = [];
      }
      resultsByTest[result.testName].push(result);
    }

    let totalSuccess = 0;
    let totalSamples = 0;

    // Print results by test and provider
    for (const [testName, results] of Object.entries(resultsByTest)) {
      console.log(`\nTest: ${testName}`);

      for (const result of results) {
        console.log(`  ${result.provider} (${result.modelId}):`);
        console.log(
          `    pass@1: ${result.pass1.toFixed(4)}${
            result.numSamples > 1 ? `, pass@10: ${result.pass10.toFixed(4)}` : ""
          }`
        );
        console.log(
          `    Samples: ${result.numSamples}, Correct: ${result.numCorrect}`
        );

        totalSuccess += result.numCorrect;
        totalSamples += result.numSamples;
      }
    }

    console.log("\n===========================================");
    console.log(
      `Total Samples: ${totalSamples}, Passed: ${totalSuccess}, Failed: ${
        totalSamples - totalSuccess
      }`
    );

    // Clean up all tmp directories - carefully
    await cleanTmpDir();
    for (const providerName of providerNames) {
      await cleanTmpDir(providerName);
    }

    // Exit with appropriate code
    const exitCode = totalSuccess > 0 ? 0 : 1;
    process.exit(exitCode);
  } catch (error) {
    console.error("Error running benchmark:", error);
    process.exit(1);
  }
}

// Run the benchmark
runBenchmark().catch((error) => {
  console.error("Unhandled error:", error);
  process.exit(1);
});
