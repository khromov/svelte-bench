# SvelteBench Wiki

Welcome to the SvelteBench Wiki! This comprehensive documentation covers everything you need to know about using, contributing to, and understanding SvelteBench.

## What is SvelteBench?

SvelteBench is an LLM benchmark tool for Svelte 5 components based on the HumanEval methodology. It evaluates LLM-generated Svelte components by testing them against predefined test suites and calculates pass@k metrics.

## Quick Navigation

### Getting Started
- **[Getting Started](Getting-Started.md)** - Detailed setup and installation guide
- **[Examples](Examples.md)** - Usage examples and common scenarios

### Technical Documentation
- **[Architecture](Architecture.md)** - System design and technical overview
- **[API Reference](API-Reference.md)** - Detailed API documentation

### Contributing
- **[Contributing](Contributing.md)** - Development guidelines and contribution process

### Support
- **[FAQ](FAQ.md)** - Frequently asked questions
- **[Troubleshooting](Troubleshooting.md)** - Common issues and solutions

## Key Features

- **Multiple LLM Providers**: Support for OpenAI, Anthropic, Google, and OpenRouter
- **HumanEval Methodology**: Industry-standard evaluation approach
- **Pass@k Metrics**: Comprehensive performance measurement
- **Sequential & Parallel Execution**: Flexible execution modes
- **Checkpointing**: Resume interrupted benchmarks
- **Context Support**: Include documentation for better LLM performance

## Recent Updates

For the latest changes and updates, see the main [README.md](../README.md) in the repository root.

## Getting Help

- Check the [FAQ](FAQ.md) for common questions
- Review [Troubleshooting](Troubleshooting.md) for technical issues
- Open an issue on GitHub for bugs or feature requests
- Join discussions for general questions

---

*This wiki is a work in progress. Contributions and improvements are welcome!*